{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cc.zh.300.bin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import KNNImputer\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('zh', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- normalize embedding?\n",
    "- concat all embedding -> clustering (use ankle/silhouette method to choose best K)\n",
    "- Kmeans ref:\n",
    "    - [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "    - [silhouette_score](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130566, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = '../hahow/data/'\n",
    "user_csv_path = os.path.join(DATA_ROOT, 'users.csv')\n",
    "user_df = pd.read_csv(user_csv_path)\n",
    "user_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_embed = pd.get_dummies(user_df['gender']).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### occupation\n",
    "- TODO\n",
    "    - number of occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130566, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.zh.300.bin')\n",
    "occu_embed = np.zeros((user_df.shape[0], 300))\n",
    "occu_embed[:] = np.nan\n",
    "occu_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, occus in enumerate(user_df['occupation_titles']):\n",
    "    # not nan\n",
    "    if type(occus) == str:\n",
    "        if ',' in occus:\n",
    "            occus = occus.split(',')\n",
    "        else:\n",
    "            occus = [occus]\n",
    "\n",
    "        word_vec = np.zeros((1, 300))\n",
    "        word_vec[:] = np.nan\n",
    "        for occu in occus:\n",
    "            word_vec += ft.get_word_vector(occu)\n",
    "    # give (0, ..., 0) to nan \n",
    "    else:\n",
    "        word_vec = np.zeros((1, 300))\n",
    "        word_vec[:] = np.nan\n",
    "    occu_embed[i, :] = word_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_int_embed, sub_int_embed = np.zeros((user_df.shape[0], 300)), np.zeros((user_df.shape[0], 300))\n",
    "main_int_embed[:], sub_int_embed[:] = np.nan, np.nan\n",
    "for i, interest in enumerate(user_df['interests']):\n",
    "    if type(interest) == str:\n",
    "        # user has serveral interests\n",
    "        if ',' in interest:\n",
    "            int_split = interest.split(',')\n",
    "            main_int_set = set()\n",
    "            for int_ in int_split:\n",
    "                main_int = int_.split('_')[0]\n",
    "                sub_int = int_.split('_')[1]\n",
    "                main_int_set.update([main_int])\n",
    "                sub_int_embed[i, :] += ft.get_word_vector(sub_int)\n",
    "\n",
    "            # mulitple main interest only counts once\n",
    "            for main_int in main_int_set:\n",
    "                main_int_embed[i, :] += ft.get_word_vector(main_int)\n",
    "        \n",
    "        # user has only one interest\n",
    "        else:\n",
    "            main_int = interest.split('_')[0]\n",
    "            sub_int = interest.split('_')[1]\n",
    "\n",
    "            main_int_embed[i, :] += ft.get_word_vector(main_int)\n",
    "            sub_int_embed[i, :] += ft.get_word_vector(sub_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130566, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_embed = np.zeros((user_df.shape[0], 300))\n",
    "rec_embed[:] = np.nan\n",
    "rec_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, recs in enumerate(user_df['recreation_names']):\n",
    "    # not nan\n",
    "    if type(recs) == str:\n",
    "        if ',' in recs:\n",
    "            recs = recs.split(',')\n",
    "        else:\n",
    "            recs = [recs]\n",
    "\n",
    "        word_vec = np.zeros((1, 300))\n",
    "        word_vec[:] = np.nan\n",
    "        for occu in recs:\n",
    "            word_vec += ft.get_word_vector(occu)\n",
    "    # give (0, ..., 0) to nan \n",
    "    else:\n",
    "        word_vec = np.zeros((1, 300))\n",
    "        word_vec[:] = np.nan\n",
    "    rec_embed[i, :] = word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130566, 1203)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embed = np.concatenate(\n",
    "    [gender_embed, occu_embed, main_int_embed, sub_int_embed, rec_embed],\n",
    "    axis=1\n",
    ")\n",
    "# (number of users, 1203)\n",
    "user_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=10)\n",
    "imputer.fit_transform(user_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 2, start clustering\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, kmax\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, start clustering\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m      9\u001b[0m     sil\u001b[38;5;241m.\u001b[39mappend(silhouette_score(user_embed, labels, metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/adl_final/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1426\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1417\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m   1418\u001b[0m     X,\n\u001b[1;32m   1419\u001b[0m     accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1424\u001b[0m )\n\u001b[0;32m-> 1426\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_params_vs_input(X)\n\u001b[1;32m   1428\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[1;32m   1429\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/adl_final/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1362\u001b[0m, in \u001b[0;36mKMeans._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_params_vs_input\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m-> 1362\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_check_params_vs_input(X, default_n_init\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m   1364\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_algorithm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm\n\u001b[1;32m   1365\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_algorithm \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/adl_final/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:864\u001b[0m, in \u001b[0;36m_BaseKMeans._check_params_vs_input\u001b[0;34m(self, X, default_n_init)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    860\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mn_samples=\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m should be >= n_clusters=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_clusters\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    861\u001b[0m     )\n\u001b[1;32m    863\u001b[0m \u001b[39m# tol\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tol \u001b[39m=\u001b[39m _tolerance(X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol)\n\u001b[1;32m    866\u001b[0m \u001b[39m# n-init\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[39m# TODO(1.4): Remove\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_init \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_init\n",
      "File \u001b[0;32m~/anaconda3/envs/adl_final/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:269\u001b[0m, in \u001b[0;36m_tolerance\u001b[0;34m(X, tol)\u001b[0m\n\u001b[1;32m    267\u001b[0m     variances \u001b[39m=\u001b[39m mean_variance_axis(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     variances \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvar(X, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    270\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(variances) \u001b[39m*\u001b[39m tol\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mvar\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/adl_final/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3621\u001b[0m, in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3618\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3619\u001b[0m         \u001b[39mreturn\u001b[39;00m var(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, ddof\u001b[39m=\u001b[39mddof, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3621\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_var(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout, ddof\u001b[39m=\u001b[39;49mddof,\n\u001b[1;32m   3622\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/adl_final/lib/python3.8/site-packages/numpy/core/_methods.py:205\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    202\u001b[0m x \u001b[39m=\u001b[39m asanyarray(arr \u001b[39m-\u001b[39m arrmean)\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(arr\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, (nt\u001b[39m.\u001b[39mfloating, nt\u001b[39m.\u001b[39minteger)):\n\u001b[0;32m--> 205\u001b[0m     x \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39;49mmultiply(x, x, out\u001b[39m=\u001b[39;49mx)\n\u001b[1;32m    206\u001b[0m \u001b[39m# Fast-paths for built-in complex types\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39melif\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m _complex_to_float:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sil = []\n",
    "kmax = 10\n",
    "\n",
    "# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "for k in range(2, kmax+1):\n",
    "    print(f\"K: {k}, start clustering\")\n",
    "    kmeans = KMeans(n_clusters = k).fit(user_embed)\n",
    "    labels = kmeans.labels_\n",
    "    sil.append(silhouette_score(user_embed, labels, metric = 'euclidean'))\n",
    "    print(f\"K: {k}, finish clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label + user id preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59737, 2),\n",
       "                     user_id                                    subgroup\n",
       " 0  5bdecbfffec014002166796a                                          27\n",
       " 1  5fedf958af850a915c86362c  1 7 19 29 36 49 50 51 59 61 63 64 66 69 72\n",
       " 2  5fd255c43136a460c6f3f930                                        8 28\n",
       " 3  5a0bde2aa15b3f001e98429a                               1 59 60 71 79\n",
       " 4  5fedf8132a0eb0bfab27882b                                          89)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_group_csv_path = os.path.join(DATA_ROOT, 'train_group.csv')\n",
    "train_group_csv = pd.read_csv(train_group_csv_path)\n",
    "train_group_csv.shape, train_group_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2embed = dict()\n",
    "for i, id in enumerate(user_df['user_id']):\n",
    "    id2embed[id] = user_embed[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234597"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label_cnt = 0\n",
    "for sub_label in train_group_csv['subgroup']:\n",
    "    if type(sub_label) == str:\n",
    "        if ' ' in sub_label:\n",
    "            sub_label_split = sub_label.split(' ')\n",
    "            all_label_cnt += len(sub_label_split)\n",
    "        else:\n",
    "            all_label_cnt += 1\n",
    "all_label_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_group = np.zeros(\n",
    "    (all_label_cnt, user_embed.shape[1]+1)\n",
    ")\n",
    "cnt = 0\n",
    "for i, data in train_group_csv.iterrows():\n",
    "    sub_label = data['subgroup']\n",
    "    if type(sub_label) == str:\n",
    "        user_feat = id2embed[data['user_id']]\n",
    "        if ' ' in sub_label:\n",
    "            sub_label_split = sub_label.split(' ')\n",
    "            for sub_label in sub_label_split:\n",
    "                X_train_group[cnt, :user_embed.shape[1]] = user_feat\n",
    "                X_train_group[cnt, user_embed.shape[1]:] = int(sub_label)\n",
    "                cnt += 1\n",
    "        else:\n",
    "            X_train_group[cnt, :user_embed.shape[1]] = user_feat\n",
    "            X_train_group[cnt, user_embed.shape[1]:] = int(sub_label)\n",
    "            cnt += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_group, y_train_group = \\\n",
    "    X_train_group[:, :user_embed.shape[1]], X_train_group[:, user_embed.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((234597, 903), (234597, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_group.shape, y_train_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r11922a05/anaconda3/envs/adl_final/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(\n",
    "    X_train_group, y_train_group.flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"multi:softprob\", \n",
    "    random_state=42)\n",
    "# y should start in zero \n",
    "xgb_model.fit(X_train_group, y_train_group-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seen_csv_path = os.path.join(DATA_ROOT, './test/test_seen_group.csv')\n",
    "test_unseen_csv_path = os.path.join(DATA_ROOT, './test/test_unseen_group.csv')\n",
    "test_seen_df = pd.read_csv(test_seen_csv_path)\n",
    "test_unseen_df = pd.read_csv(test_unseen_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seen = np.zeros((test_seen_df.shape[0], user_embed.shape[1]))\n",
    "X_unseen = np.zeros((test_unseen_df.shape[0], user_embed.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, id in enumerate(test_seen_df['user_id']):\n",
    "    X_seen[i, :] = id2embed[id]\n",
    "\n",
    "for i, id in enumerate(test_unseen_df['user_id']):\n",
    "    X_unseen[i, :] = id2embed[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 50\n",
    "pred_seen = xgb_model.predict_proba(X_seen)\n",
    "# pred_seen = clf.predict_proba(X_seen)\n",
    "pred_top_50 = pred_seen.argsort()[:, ::-1] \\\n",
    "    [:, :50] + 1 # subgroup id start from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_top_50_list = pred_top_50.tolist()\n",
    "preds_seen = [' '.join(\n",
    "    map(str, pred)\n",
    "    ) for pred in pred_top_50_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_seen_df = pd.DataFrame(\n",
    "    {\n",
    "        'user_id': test_seen_df['user_id'],\n",
    "        'subgroup': preds_seen\n",
    "    }\n",
    ")\n",
    "preds_seen_df.to_csv('./pred_record/1217/seen_group_1217_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 50\n",
    "pred_unseen = xgb_model.predict_proba(X_unseen)\n",
    "# pred_unseen = clf.predict_proba(X_unseen)\n",
    "pred_top_50 = pred_unseen.argsort()[:, ::-1] \\\n",
    "    [:, :50] + 1 # subgroup id start from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_top_50_list = pred_top_50.tolist()\n",
    "preds_unseen = [' '.join(\n",
    "    map(str, pred)\n",
    "    ) for pred in pred_top_50_list]\n",
    "preds_unseen_df = pd.DataFrame(\n",
    "    {\n",
    "        'user_id': test_unseen_df['user_id'],\n",
    "        'subgroup': preds_unseen\n",
    "    }\n",
    ")\n",
    "preds_unseen_df.to_csv('./pred_record/1217/unseen_group_1217_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('adl_final')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddbe84b8b3ed712b164a7afcb60a90ff35fb7c6bf278ea210bca27b38500523e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
